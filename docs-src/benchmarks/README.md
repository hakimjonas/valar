# Valar Benchmarking Suite

Valar includes a comprehensive benchmarking strategy to ensure optimal performance across platforms and use cases.

## Benchmark Tiers

The benchmarking suite is organized into four tiers of increasing sophistication:

### Tier 1: Core Performance Benchmarks

These benchmarks cover the essential validation paths:

- **Synchronous validation** of simple and nested case classes
- **Asynchronous validation** with a mix of sync and async rules
- **Valid and invalid data paths** to understand performance differences

### Tier 2: Extended Benchmark Suite

These benchmarks cover more sophisticated validation scenarios:

- **Macro Derivation Overhead**: Measures the performance impact of macro-generated validators
- **Error Accumulation Scaling**: Tests performance with many validation errors
- **Observer Pattern**: Measures the overhead of using the observer pattern
- **Translator Integration**: Benchmarks i18n translation integration

### Tier 3: Cross-Platform Benchmarks

These benchmarks ensure consistent performance across platforms:

- **Native Benchmarks**: Custom timing-based benchmarks for Scala Native
- **Performance Regression Detection**: Automated benchmarking in CI
- **Platform Comparison Reports**: JVM vs Native performance analysis

### Tier 4: Advanced Benchmarking

These benchmarks measure advanced performance characteristics:

- **Memory Allocation Profiling**: Tracks GC pressure and allocation patterns
- **Concurrency Benchmarks**: Tests async validation performance
- **Stress Testing**: Measures performance with deeply nested structures

## Benchmark Results

### JVM Platform Results

Based on the latest run (JDK 21.0.7, OpenJDK 64-Bit Server VM):

| Benchmark            | Mode | Score      | Error       | Units |
|----------------------|------|------------|-------------|-------|
| `syncSimpleValid`    | avgt | 44.628     | ¬± 6.746     | ns/op |
| `syncSimpleInvalid`  | avgt | 149.155    | ¬± 7.124     | ns/op |
| `syncNestedValid`    | avgt | 108.968    | ¬± 7.300     | ns/op |
| `syncNestedInvalid`  | avgt | 449.783    | ¬± 18.373    | ns/op |
| `asyncSimpleValid`   | avgt | 13,212.036 | ¬± 1,114.597 | ns/op |
| `asyncSimpleInvalid` | avgt | 13,465.022 | ¬± 214.379   | ns/op |
| `asyncNestedValid`   | avgt | 14,513.056 | ¬± 1,023.942 | ns/op |
| `asyncNestedInvalid` | avgt | 15,432.503 | ¬± 2,592.103 | ns/op |

### Native Platform Results

Latest Scala Native benchmark results:

| Benchmark                    | Time (ns) | Platform |
|------------------------------|-----------|----------|
| syncSimpleValid              | 9877.158  | Native   |
| syncSimpleInvalid            | 7839.325  | Native   |
| syncNestedValid              | 12510.448 | Native   |
| syncNestedInvalid            | 15844.767 | Native   |
| macroDerivationOverhead      | 10981.036 | Native   |
| macroDerivationOverheadInvalid| 8662.287  | Native   |
| errorAccumulationScaling     | 89865.935 | Native   |
| errorAccumulationScalingValid| 11353.067 | Native   |
| observerPattern              | 871.084   | Native   |

## Performance Analysis

### üöÄ Synchronous Performance

The validation for simple, valid objects on JVM completes in **~45 nanoseconds**. This is incredibly fast and proves that for the "happy path," the library adds negligible overhead. The slightly higher numbers for invalid and nested cases (~150‚Äì450 ns) are also excellent and are expected, as they account for:

- Creation of `ValidationError` objects for invalid cases
- Recursive validation calls for nested structures
- Error accumulation logic

On Scala Native, the synchronous performance metrics are in the nanosecond range but generally higher than JVM due to differences in optimization and runtime characteristics.

**Key takeaway**: Synchronous validation is extremely fast with minimal overhead on both platforms.

### ‚ö° Asynchronous Performance

The async benchmarks on JVM show results in the **~13‚Äì16 microsecond range** (13,000-16,000 ns). This is excellent and exactly what we should expect. The "cost" here is not from our validation logic but from the inherent overhead of:

- Creating `Future` instances
- Managing the `ExecutionContext`
- The `Await.result` call in the benchmark (blocking on async results)

**Key takeaway**: Our async logic is efficient and correctly builds on Scala's non-blocking primitives without introducing performance bottlenecks.

### üîç Observer Pattern Overhead

The observer pattern benchmark shows minimal overhead at just **~871 nanoseconds** on Native, demonstrating that our core extensibility pattern adds negligible performance impact.

### üìä Error Accumulation Scaling

When accumulating errors in complex validation scenarios, the performance scales linearly with complexity. The error accumulation benchmark shows increased time (**~89,866 ns**) compared to valid paths (**~11,353 ns**) as expected due to error object creation and accumulation.

### üîÑ Platform Comparison

While the absolute numbers differ between JVM and Native platforms (with JVM generally showing better performance for simple operations), both platforms demonstrate excellent validation performance suitable for production use cases.

## Running Benchmarks

Valar provides a unified benchmark runner that can run benchmarks, generate reports, and analyze performance trends.

### JVM Benchmarks

The JVM benchmarks use JMH (Java Microbenchmark Harness) for accurate measurements:

```bash
sbt "valarBenchmarks / run jvm"
```

To run specific benchmarks, use JMH directly with a pattern:

```bash
sbt "valarBenchmarks / Jmh / run .*syncSimple.*"

# Run only sync benchmarks
sbt "valarBenchmarks / Jmh / run .*sync.*"

# Run only async benchmarks
sbt "valarBenchmarks / Jmh / run .*async.*"

# Run only valid cases
sbt "valarBenchmarks / Jmh / run .*Valid.*"
```

### Native Benchmarks

The Native benchmarks use a custom timing-based framework:

```bash
sbt "valarBenchmarks / run native"
```

### Generating Reports

To generate a cross-platform comparison report:

```bash
sbt "valarBenchmarks / run report"
```

### Visualization Dashboard

To generate an interactive visualization dashboard:

```bash
sbt "valarBenchmarks / run dashboard"
```

### Performance Trend Analysis

To analyze historical performance trends:

```bash
sbt "valarBenchmarks / run trends"
```

### Run All Benchmarks and Reports

To run all benchmarks and generate all reports:

```bash
sbt "valarBenchmarks / run all"
```

## Benchmark Details

### Benchmark Configuration

The benchmarks are configured with:
- **Warmup**: five iterations, 1 second each
- **Measurement**: five iterations, 1 second each
- **Fork**: 1 fork
- **Mode**: Average time (ns/op)
- **Threads**: 1 thread

### Test Models

The benchmarks use the following test models:

```scala
case class SimpleUser(name: String, age: Int)
case class NestedCompany(name: String, owner: SimpleUser)
```

With validation rules:
- `name` must be non-empty
- `age` must be non-negative

### Understanding Results

- **ns/op**: Nanoseconds per operation (lower is better)
- **Error**: 99.9% confidence interval
- **Mode avgt**: Average time across all iterations

### Advanced Profiling

For deeper performance analysis, you can use JMH's built-in profilers:

```bash
# CPU profiling
sbt "valarBenchmarks / Jmh / run -prof comp"

# Memory allocation profiling  
sbt "valarBenchmarks / Jmh / run -prof gc"

# Stack profiling
sbt "valarBenchmarks / Jmh / run -prof stack"
```

## CI Integration

The benchmarks are integrated into CI via GitHub Actions:

- Benchmarks run automatically on PRs and main branch pushes
- Performance regressions are detected by comparing with previous runs
- Platform comparison reports are generated automatically
- Interactive visualization dashboards are created for each run
- Historical performance trends are analyzed and reported
- The CI build fails if significant performance regressions are detected

## Performance Regression Policy

A performance regression is detected when:

- A benchmark is at least 10% slower than the previous run
- The regression is consistent across multiple runs

When a regression is detected:

1. The CI build will fail with details about the regression
2. The regression should be investigated and fixed before merging

## Adding New Benchmarks

To add new benchmarks:

1. Add your benchmark method to `ValarBenchmark.scala`
2. Annotate it with `@Benchmark`
3. Ensure it returns a meaningful value to prevent dead code elimination
4. Follow the existing naming conventions (`sync`/`async` + `Simple`/`Nested` + `Valid`/`Invalid`)

## Benchmark Design Principles

Valar's benchmarks follow these principles:

1. **Realistic Scenarios**: Benchmarks reflect real-world usage patterns
2. **Comprehensive Coverage**: All critical paths are benchmarked
3. **Cross-Platform**: Both JVM and Native platforms are tested
4. **Regression Detection**: Changes that degrade performance are caught early
5. **Reproducibility**: Benchmarks are stable and produce consistent results
